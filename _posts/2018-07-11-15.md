---
layout: post
title:  "Neural Laguage Models"
date:   2018-07-11 13:47:35 +0900
categories: NLP
tag: NLP
---

### Neural Language Models

이 내용은 Newyork University의 조경현교수님의 DS-GA 3001강의 lecture note중 Neural Laguage Models단원을 정리한 내용입니다.


<br>
#### Language Modeling

Machine이 우리의 언어를 이해하는 방법은 어떻게 될까?
가장 basic한 방법으로는 문장들이 얼마나 유사(Likeliness)한지에 대해 이해하는 것이다. 아래의 두 문장을 보자.

> "Colorless green ideas sleep furiously"  
> "Jane and me went to see a movie yesterday"

첫 번째 문장은 문법적으로 완벽한 문장이다. 하지만 문장을 사람이 이해하려해도 어떤 의미인지 쉽게 받아 들여지지 않는다. 단어들끼리의 유사도가 매우 안맞는 문장이다. 그에 반해 두 번째 문장은 문법적으로는 잘못된 문장이다. me 대신 i가 들어가야 하지만, 저 문장을 이해하는데는 어려움이 없이 쉽게 이해 된다.

따라서 우리는 기계에게 문장을 이해시키는 방법으로 유사도를 통한 의미적인 부분을 학습시킬 것이다. 물론, 문법적인 부분도 해야 하겠지만 이는 어려운 문제이다.


#### 만약 언어학적인 구조가 존재한다면?

Language Modeling에 대해 통계학적 접근법은 문법적으로 완벽한 문장을 유사하지 않다고 결론 지을 수 있다. 이러한 문제는 문장들간의 유사도에 대해서 다루는 문제를 확률 모델로 만들 수 있을 것이다. 주어진 문장 $S$가 있다고 하자, 이때 확률 $P(S)$는 어떻게 구할 것인가?
(여기서 $P(S)$는 간략하게 유사도와 문법적 정확도를 같게 보는 것으로 해석할 것이다.)
 우선 $S$에 의해 만들어지는 잠재적 언어학 구조를 $G$라 가정하자. 아래와 같은 식을 얻을 수 있다.

$$p(S,G) = p(S|G)p(G)$$

$$p(S)=\sum_G p(S, G)$$

하지만 여기서 $G$는 정해진 것이 아니므로, 무한할 수 있다. 따라서 실제 계산을 위해서는 Lower bound를 사용해 아래와 같은 식을 사용해 $p(S)$를 근사한다.

$$p(S)=\sum_G p(S, G) \ge p(S,\hat G)$$

$$where\;\hat G = argmax_G p(S,G) = argmax_G p(G|S)$$

하지만 이러한 모델링에도 문제점이 있다. 첫 째로, $G$를 사용하는 것에 대해 명백하지 않다. 둘 째로는 이러한 형식들이 의미적으로 불확실하다.

이제부터는 model-free한 접근법에 대해서 알아볼것이다.

#### Quick Note on Linguistic Units

 *"언어학적 최소 단위가 무었일까?"*
위 질문에 대한 대답은 음소(phoneme)가 될 것이다. 하지만 "음소가 언어를 이해할 수 있는 단계인가?" 에 대해서는 아니라고 할 것이다.
이러한 low-level의 unit들 (음소, 글자)는 의미를 수반하지 않기 떄문에 이해할 수 있는 단계가 아니다. 따라서 우리는 한 단어를 최소 단위로 의미를 이해할 수 있다. 그러나 여기에도 문제가 있다. '어떤것을 단어로 볼 것인가?' 보통은 띄어쓰기(sequence of non-blank characters)를 기준으로 단어를 구분한다. 그러나 구두점과 같은 것들이 나오면 단어에 대한 정의가 애매해진다.
> "hello," , "hello.", "hello!", "hello?", ""hello"", "'hello'"
> 위의 hello들은 모두 같은 단어이지만 구두점 때문에 다른 의미로 해석될 수 있다.

뿐만 아니라 중국어 같은 언어에서는 띄어쓰기가 사용되지 않아서 위와 같은 정의를 적용시킬 수 없다.
따라서 우리는 이 최소단위에 대한 정의에 대한 고민을 해야 할 것이다. 이 분야는 물론 지금도 계속 연구되고 있고 답이 없는 분야이다. 즉, character와 word 사이의 적절한 liguistic unit이 있는지를 찾아야 한다.

#### Statical Laguage Model

Liguistic unit에 상관없이, 어떤 문장$S$는 $T$개의 각각의 symbol들로 구분될 수 있다.

$$S=(w_1, w_2, ... ,w_T)$$

각 symbol들은 vocabulary라는 가능한 모든 symbol이 모여있는 집합의 원소이다.

$$V= \{v_1, v_2, ... , v_{|V|}\}$$

Laguage Modeling 문제는 Sentence에 대해 확률 $P(S)$를 할당하는 model을 찾는 문제이다.
그러나 우리는 확률 분포에 대한 정보가 없을 것이고, Data로 부터 학습을 해야 할 것이다.
$D$를 $N$개의 Sentence를 가지고 있는 Data라 하자.

$$D=\{S^1, S^2, ... S^N\}$$

$$S^n = (w_1^n, w_2^n, ... w_{T^n}^n)$$

$T^n$이라는 표현은 각 sentence들의 길이가 다르다는 것을 의미한다.

이제 주어진 Data $D$에 대해 Sentence $S$의 확률을 다음과 같이 정의하자.

$$p(S) = \frac{\sum_n^NI_{S=S^n}}{N} $$

여기서 $I$는 indicator function으로 동일한 Sentence면 1, 아니면 0을 주는 함수이다.

$$I_{S=S^n}=\bigg\lbrace\begin{matrix} 1, & \text{if}\ \ S=S^n \\ 0, & \text{otherwise} \end{matrix}$$

위의 확률은 sentence $S$가 Data 내에서 몇번 나왔는지와 같다.

#### Data Sparsity/Scarcity

Laguage model에서 가장 중요한 issue는 corpus의 크기가 얼마나 큰가이다. 즉 dataset의 크기가 중요한데, 실제 세계의 사용가능한 모든 문장을 data로 가지고 있을 수는 없다.
$|V|$개의 symbol을 가지는 vocabulary가 있다고 하자. 각 sentence의 최소한 $T$개의 symbol가진다. 이러한 vocabulary는 충분히 큰 크기가 될 것이다(100k ~ 1M). 그럼에도 불구하고, 많은 실제로 사용할만한 문장임에도 불구하고 corpus에 포함되지 않을 수 있다.
그 예를 한번 찾아보자. Google Books Ngram Viewer라는 서비스는 Google Books의 문장들이 모두 모여있는 거대한 corpus를 나타낸다. 여기서 "I like llama"라는 문장을 검색해보자, 이 문장은 충분히 사용할만한 문장임에도 불구하고 결과가 나오지 않는다.
![3123123](https://i.imgur.com/keV0Uh7.jpg)
이전 part에서 정의한 sentence에 대한 확률을 생각해보자, "i like llama"라는 문장은 corpus에 존재하지만, 많이 등장하지는 않을 것이다. 따라서 데이터셋이 매우 크므로 확률은 거의 0에 수렴할 것이다. 이러한 문제를 *data sparsity*라 부른다.
즉 trainning set이 전체 input space를 cover하지 못하는 경우를 뜻한다.


**n-Gram Language Model**

Data Sparsity 문제는 sentence의 최대 길이가 커질수록 악화된다. 우리는 이 사실에 착안해서 straightforward한 접근법을 사용한다. (*limit the maximum length of phrases/sentences we estimate a probability on*)
이러한 idea는 n-Gram language model의 base가 되었다.
n-Gram 모델에서는 위에서 정의한 sentence에 대한 확률을 재정의한다.

$$p(S)=p(w_1,w_2,...w_T)=p(w_1)p(w_2|w_1)\dotsm p(w_k|w_{<k})\dotsm p(w_T|w_{<T})$$

여기에서 $w_{<k}$는 k 번째 symbol 이전의 모든 symbol들을 뜻한다. 아래의 식과 같다.

$$p(w_k|w_{<k})\approx p(w_k|w_{k-n},w_{k-n+1},...,w_{k-1})$$

따라서 Sentence의 확률이 다음과 같이 정의된다.

$$p(S)\approx \prod_{t=1}^Tp(w_t|w_{t-n},...w_{t-1})$$

위 식이 의미하는 것은 sentence내의 symbol은 이전의 $n-1$개의 symbol들에 의해 predictable하다는 것이다.
예를 들어 "In Korea, more than half of all the residents speak Korean" 문장을 보면 마지막의 'Korean'이라는 단어는 사람과 언어 두가지 의미를 가지는데, 그 이전의 speak과 Korea라는 단어에 대해 상대적인 확률로 생각을 해보면 언어를 뜻하는 것이 더 타당하다는 것을 알 수 있다. 즉 n-Gram 모델을 사용하면 이전의 symbol에 대한 정보를 가지고 현재 symbol의 의미를 파악하기 때문에 유용하다는 것을 알 수 있다.
위의 예를 통해 보면, 확률 추정의 정확도와 $n$의 크기에 따른 통계적인 효율은 trade-off 관계에 있다는 것을 알 수 있다. 즉, $n$의 값이 커질수록, conditional distribution은 더 나은 결과를 만들 것이지만 data sparsity는 커져 정확도가 떨어질 수 있다.M

* *n-gram Probablity Estimate*

n-gram conditional probabilty는 trainning corpus를 통해 계산할 수 있다. 아래의 식을 보자.

$$p(w_k|w_{k-n},w_{k-n+1},...,w_{k-1})=\frac{p(w_{k-n},...w_{k-1},w_k)}{p(w_{k-n},...,w_{k-1})}$$

여기서 분모는 다음과 같이 계산할 수 있다.

$$p(w_{k-n},...,w_{k-1})=\sum_{w^\prime\in V}p(w_{k-n},...,w_{k-1},w^\prime)$$

$$p(w_{k-n},...,w_{k-1},w^\prime)\approx \frac{c(w_{k-n},...w_{k-1},w^\prime)}{N_n}$$

여기서 $c(\cdot)$가 의미하는 것은 trainning corpus에서 주어진 n-gram의 등장 횟수이다. 그리고 $N_n$은 trainning corpus에서의 모든 n-gram의 수 이다.
마지막으로 다음과 같이 식을 정리한다.

$$p(w_k|w_{k-n},...,w_{k-1})=\frac{\cancel{\frac{1}{N_n}}c(w_{k-n},...,w_{k-1},w^\prime)} {\cancel{\frac{1}{N_n}}\sum_{w^\prime\in V}c(w_{k-n},...w_{k-1},w^\prime)}$$


#### Smoothing and Back-Off

n-gram model의 가장 큰 issue는 어떤 n-gram을 포함하는 sentence에 대해 얼마다 다른 n-gram들과 유사한지와 상관없이 0의 확률을 갖는 것이다.
예시를 통해 이러한 경우에 대해 살펴보자.
* "I like llama which is a domesticated South American camelid"

위의 문장에 대한 확률은 다음과 같이 계산될 것이다.

$$\begin{align*}
&p(\text{"I","like","llama","which","is","a","domesticated","South","American","camelid"}\\
&=p("\text{I}")p("\text{like}"|"\text{I}")\underbrace{p("\text{llama}"|"\text{I}","\text{like}")}_{=0}\dotsm p("\text{camelid}"|"\text{South}","\text{American}")\\
&=0
\end{align*}
$$

위 식과 같이 특정 n-gram 확률 때문에 전체가 0이되는 문제를 해결하기 위해 corpus의 크기를 키우면 되지 않을까 라고 생각한다. 하지만 이러한 "data sparsity" 문제는 corpus의 크기와 상관없이 statistical modeling에서 항상 발생하는 문제이다. 그렇다면 어떻게 해결해야 할 것인가?
방법은 기존에 존재하지 않는 n-gram에 대해 작은 특정 값을 갖도록 하는 것이다. 그러면 기존에 나오지않은 n-gram이더라도 특정 값을 가지므로 0이 되는 문제는 발생하지 않을 것이다. 식을 아래와 같이 구현함으로써 이 방법을 사용할 수 있다.

$$
\begin{align*}
p(w_k|w_{k-n},...,w_{k-1})&=&\frac{\alpha+c(w_{k-n},...,w_{k})}{\sum_{w^{\prime}\in V}(\alpha+c(w_{k-n},...,w^\prime))}\\
&=&\frac{\alpha+c(w_{k-n},...,w_{k})}{\alpha|V|+\sum_{w^{\prime}\in V}c(w_{k-n},...,w^\prime)}
\end{align*}
$$

$\alpha$는 scalar 값으로 보통 다음의 범위로 정한다. $0<\alpha\le1$

 *unseen n-gram* 을 위와 같이 해결하는 방법을 smoothing 기법(부족한 데이터를 해결하는 방법)이라 표현하고 위와 같이 특정 scalar값을 더해주는 방법을 Additive Smoothing 혹은 Lidstone Smoothing이라고 표현한다. 그 중 $\alpha$값으로 1을 지정하는 방법은 Laplace Smoothing이라고 한다.(Laplace는 한번도 안나온 데이터에 대해 최소 한번은 나왔다고 지정하는 방법)
 하지만 직관적으로 봐도 위 방법은 효율일 잘 안나올 것 같다. unseen n-gram들에 대해 모두 다 똑같은 빈도수를 준다는 점에서 문제가 발생한다.

 이에 대한 해결책은 n-gram 확률을 interpolation 방법을 통해 smoothing하는 것이다. 수식은 다음과 같이 정의한다.

$$
begin{align*}
p^S(w_k|w_{k-n},...,w_{k-1})=&\lambda&(w_{k-n},...,w_{k-1})p(w_k|w_{k-n},...,w_{k-1})\\
&+&(1-\lambda(w_{k-n},...,w_{k-1}))p^S(w_k|w_{k-n+1},...,w_{k-1})
end{align*}
$$   

위 식에 따르면 n-gram 확률은 이전 n-gram확률에 의해 재귀적으로 계산되는 것을 볼 수 있다. 이제 중요한 것은 $\lambda$값을 어떻게 지정해야할까 이다. 간단한 방법은 데이터에 맞추는 방법이다. 즉 데이터에서 출현빈도에 초점을 맞추는 방법이다. $\lambda$값까지 정한뒤 일반화하면 다음과 같다.

$$
p^S(w_k|w_{k-n},...,w_{k-1})=
\begin{cases}
\alpha(w_k|w_{k-n},...,w_{k-1}),~~\text{if}~~c(w_{k-n},...,w_{k-1},w_k)>0\\
\gamma(w_{k-n+1},...,w_k)p^S(w_k|w_{k-n+1},...,w_{k-1}),~otherwise
\end{cases}
$$

여기서 $\alpha$값과 $\gamma$값의 선택에 따라 여러 techniques으로 나뉜다.

대표적인 smoothing 방법은 KN smoothing과 GT smoothing 방법이다. 이에 대한 설명은 생략한다.

#### Lack of Generalization

n-gram 모델이 대부분의 상황에서 잘 동작하지만, 몇몇 특수한 상황에서 아쉬운 상황을 생긴다. 그 이유 중 하나는 Lack of Generalization인데, 여기서 말하는 Generalization은 아래의 예를 보며 이해를 하자.

* Chases a rabbit
* Chases a cat
* Chases a dog

세 문장을 보면 우리는 Chases라는 단어 뒤에 오는 단어에 대한 패턴을 인식할 수 있다. 즉 사람은 일정한 단어들을 보고 그것들의 상위개념들을 이해할 수 있다. 하지만 n-gram 모델에서는 이런 상위개념들로 일반화시키는 것을 할 수 없다.

만약 n-gram 모델에게 이런 일반화 과정들을 이해시키려면 각 단어의 정의에 대한 사전을 이해시키거나 각 단어의 정확한 뜻을 알려주는 방법인데 이 방법을 해결한다는 것은 정의를 text로 알려주는 한 결국 Language를 이해시키는 것과 똑같기 때문에 불가능 하다.

다음 장에서는 이런 Lack of Generalization문제를 어느정도 해결하는 방법에 대해서 살펴보도록 하자.

#### Neural Language Model

n-gram 모델은 현재 단어 이전의 n개의 단어에 대해 현재 단어에 대한 조건부 확률을 계산하는 것이였다. 즉 n개의 단어에 대해 확률을 구하는 함수를 찾는 것이다.

$$
p(w_k|w_{k-n},...,w_{k-1})=f^{w_k}_\theta (w_{k-n},...,w_{k-1})
$$

위 함수를 정의하기 위해 가장 먼저 해야 할 것은 input값을 정의하는 것이다. 사전의 단어에 대한 정보를 최소화 해야하기 때문에 우리는 one-hot encoding 방식을 사용할 것이다. input은 다음과 같이 정의된다.

$$
w_i=[0, 0,...,1,...0]^\intercal\in{0,1}^{|V|}
$$


 이제 일반적인 Neural Net과 비슷한 방식으로 계산한다. 우선 간 단어 벡터에 가중치 행렬($E\in\mathbb{R}^{|V|\times d}$)을 곱해서 연속된 vector들을 구하자.


$$
p^j=E^\intercal w^j
$$

input값이 one-hot vector였던 것을 생각하자. 가중치 행렬과 곱해질 때 $w_k$는 1이 1개만 있으므로 가중치 행렬의 k번째 행과만 계산될 것이다. 따라서 가중치 행렬은 다음과 같이 각 벡터들의 모음으로 작성할 수 있다.

$$
E=
\begin{bmatrix}
e_1\\
e_2\\
\vdots\\
e_{|V|}
\end{bmatrix}
$$

따라서 input과 가중치 행렬의 곱은 다음과 같이 정의된다.

$$
E^\intercal w_i = e_i
$$

이 식이 의미하는 바는 가중치 행렬의 i행 vecter인 $e_i$는 단어들중 i번째 단어를 포현하는 벡터로 해석될 수 있다는 점이다. 따라서 $e_i$벡터를 input값으로 사용할 것이다. 이제 input값이 n개의 vector를 concat한 뒤 non-linear하게 만들어 주면 모델링 과정이 끝난다.

$$
\mathbf{p}=[p^1;p^2;...;p^n]^\intercal
$$

$$
\mathbf{h}=\tanh(\mathbf{Wp+b})
$$

최종적으로 모델링을 사용한 조건부 확률은 다음과 같이 정의된다.

$$
p(w_n=k|w_1,...w_{n-1}) =\mu_k=\frac{\exp(u_k^\intercal h+c_k)}{\sum^{|K|}_{k'=1}\exp(u_{k'}^\intercal h+c_{k'})}
$$
