---
layout: post
title:  "CS20(TensorFlow) Lecture Note (4): word2vec + manage experiments"
date:   2018-08-16 13:47:35 +0900
categories: TensorFlow
tag: tensorflow
---

스탠포드의 TensorFlow 강의인 cs20 강의의 lecture note를 정리한 글입니다. 강의는 오픈되지 않아서 Lecture note, slide 위주로 정리된 글임을 참고 해주시길 바랍니다. 강의의 자세한 Syllabus 및 자료들을 아래 링크를 참고해 주세요.

[CS20: TensorFlow for Deep Learning Research](http://web.stanford.edu/class/cs20si/)


---


Post list
* [Lecture 1, 2: Overview & TensorFlow Operation](https://reniew.github.io/32)
* [Lecture 3: Linear and Logistic Regression](https://reniew.github.io/33)
* [Lecture 4: Eager execution and interface](https://reniew.github.io/34)
* [Lecture 5: word2vec + manage experiments](https://reniew.github.io/36)

---

### 5. word2vec + manage experiments


이때까지는 간단한 모델을 만드는 방법에 대해서 알아보았다. 이번 강의에서는 이전 보다는 좀 더 복잡한 모델인인 word2vec을 예제로 모델을 만들어 보도록 한다. 이번 모델을 만들면서 variable sharing, model sharing 그리고 manage our experiments에 대해서 알아보도록 할 것이다.



#### Word2vec

단어 임베딩을 하는 방법 중에서 가장 널리 알려지고 많이 사용되는 기술은 word2vec일 것이다. 내용에 대해서는 아마 대부분이 알고 있을 것이라 생각하고 자세한 내용은 설명하지 않는다. 만약 잘 모른다면 다음의 글들을 참고하자 : [paper1](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), [paper2](https://arxiv.org/pdf/1301.3781.pdf), [blog1](https://reniew.github.io/21/), [blog2](https://reniew.github.io/22/)


word2vec의 두 가지 모델(skip-gram, CBOW)중에서 이번 강의에서는 skip-gram 모델을 구현해보도록 한다.

#### Implementing word2vec

여기서는 Session을 사용할 것이다. eager를 사용하는 모델은 `examples/04_word2vec_eager.py` 파일을 참고하자.

우선은 우리가 사용할 라이브러리들을 임포트한다.

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
from tensorflow.contrib.tensorboard.plugins import projector
import tensorflow as tf

import utils
import word2vec_utils
```

word2vec_utils은 중간에 사용되는 몇 가지 기능들을 미리 만들어 놓은 파이썬 파일이다. 그리고 다음으로는 모델의 하이퍼 파라미터를 정의하자.

```python
VOCAB_SIZE = 50000
BATCH_SIZE = 128
EMBED_SIZE = 128            # dimension of the word embedding vectors
SKIP_WINDOW = 1             # the context window
NUM_SAMPLED = 64            # number of negative examples to sample
LEARNING_RATE = 1.0
NUM_TRAIN_STEPS = 100000
VISUAL_FLD = 'visualization'
SKIP_STEP = 5000

DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'
EXPECTED_BYTES = 31344016
NUM_VISUALIZE = 3000        # number of tokens to visualize
```

우선은 이제 데이터를 다운받고 `tf.data`를 정의해야 한다. 데이터의 구조에 대해서 먼저 설명하면, skip-gram에서는 input값은 중간의 단어가 되고 output은 단어 주변의 context 단어가 된다. 하지만 여기서 구현할 때는 단어 자체를 input으로 넣지 않고 흔한 단어들에 대해서 dictionary를 만들고 input은 중간 단어에 대한 vocabulary에서의 index값을 줄 것이다. 예를 들어 만약 vocabulary에서 1000번째 단어인 경우에는 `input = 3` 이 된다.

데이터를 다운로드하고, 각 데이터를 정해진 hyperparameter에 맞게 input 값인 인덱스들을 배치사이즈로 만들어 주는 함수를 미리 정의했다. 이 함수는 `word2vec_utils.py`에 정의되어 있으며 이 과정의 세부적인 내용은 해당 파이썬 파일을 참고하자.

여기서는 해당 함수를 사용해서 데이터를 `tf.data`로 불러온 후 iterator를 정의하자.

```python
dataset = tf.data.Dataset.from_generator(gen,
                              (tf.int32, tf.int32),
                              (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))

iterator = dataset.make_initializable_iterator()
center_words, target_words = iterator.get_next()
```

skip-gram모델에서의 파라미터는 매트릭스 형태인데, 이 매트릭스의 row vector가 단어 임베딩 벡터가 된다. 따라서 매트릭스의 크기는 [VOCAB_SIZE, EMBED_SIZE]가 된다. 해당 파라미터 매트릭스는 보통 random distribution을 따르도록 초기화하는데, 여기서는 uniform distribution을 따르도록 초기화 하자.

```python
embed_matrix = tf.get_variable('embed_matrix',
                                shape=[VOCAB_SIZE, EMBED_SIZE],
                                initializer=tf.random_uniform_initializer())
```

skip-gram모델에서 단어는 원래 one-hot 인코딩 되어 있고 파라미터와 곱해질 떄 아래 그림 처럼 결국 특정 행만 계산된다. 결국 나머지는 모두 0이 됨에도 불구하고 모두 계산된다. TensorFlow에서는 이와 같은 문제를 해결하기 위한 함수인 `tf.nn.embedding_lookup`함수를 제공한다. 따라서 이 함수를 통해 batch의 단어들에 해당하는 row의 vector 값들만 사용 할 수 있다.

![http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png]

`tf.nn.embedding_lookup`함수의 구조는 아래와 같다.

```python
tf.nn.embedding_lookup(
    params,
    ids,
    partition_strategy='mod',
    name=None,
    validate_indices=True,
    max_norm=None
)
```

따라서 위의 함수를 다음과 같이 사용한다.

```python
embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')
```

이제 loss함수를 정의해야 한다. loss함수로 NCE함수를 사용할 것이다. 이미 tf에서 이 함수를 제공하고 있으므로 사용하도록 하자. NCE함수는 아래와 같이 구성되어 있다.

```python
tf.nn.nce_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=False,
    partition_strategy='mod',
    name='nce_loss'
)
```
(위 함수의 인자 중에서 3번 째가 실제로는 input이고, 4번째가 label이다)

NCE loss를 사용하기 위해 nce_weight과 nce_bias를 따로 만들어 준 후 loss 함수를 정의하자.

```python
nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],
                initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))
nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))

loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,
                                        biases=nce_bias,
                                        labels=target_words,
                                        inputs=embed,
                                        num_sampled=NUM_SAMPLED,
                                        num_classes=VOCAB_SIZE), name='loss')

```

이제 optimizer만 정의하면된다. gradient descent optimizer를 사용한다.

```python
optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)
```

이제 정의한 graph를 실행하면 된다. Session을 통해 실행하자.

```python
sess.run(iterator.initializer)
    sess.run(tf.global_variables_initializer())

    total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps
    writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)

    for index in range(NUM_TRAIN_STEPS):
        try:
            loss_batch, _ = sess.run([loss, optimizer])
            total_loss += loss_batch
            if (index + 1) % SKIP_STEP == 0:
                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))
                total_loss = 0.0
        except tf.errors.OutOfRangeError:
            sess.run(iterator.initializer)
    writer.close()
```

여기까지 하면 tensorflow로 만든 word2vec 모델이 다 끝났다. 매우 짧은 코드만으로도 복잡한 모델인 word2vec의 skip-gram을 구현했다. 코드를 다시 보면 매우 간단하지만 다시 사용하기는 어려울 것이다. 그렇다면 어떻게 해야 다시 사용하기 쉽도록 코드를 구성할 것인가?

정답은 Class 구조로 만드는 것이다. 위의 코드들을 Class구조로 만들면 다음과 같이 구성된다.

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
from tensorflow.contrib.tensorboard.plugins import projector
import tensorflow as tf

import utils
import word2vec_utils

VOCAB_SIZE = 50000
BATCH_SIZE = 128
EMBED_SIZE = 128            # dimension of the word embedding vectors
SKIP_WINDOW = 1             # the context window
NUM_SAMPLED = 64            # number of negative examples to sample
LEARNING_RATE = 1.0
NUM_TRAIN_STEPS = 100000
VISUAL_FLD = 'visualization'
SKIP_STEP = 5000

DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'
EXPECTED_BYTES = 31344016
NUM_VISUALIZE = 3000       


def word2vec(dataset):

    with tf.name_scope('data'):
        iterator = dataset.make_initializable_iterator()
        center_words, target_words = iterator.get_next()

    with tf.name_scope('embed'):
        embed_matrix = tf.get_variable('embed_matrix',
                                        shape=[VOCAB_SIZE, EMBED_SIZE],
                                        initializer=tf.random_uniform_initializer())
        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')

    with tf.name_scope('loss'):
        nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],
                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))
        nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))

        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,
                                            biases=nce_bias,
                                            labels=target_words,
                                            inputs=embed,
                                            num_sampled=NUM_SAMPLED,
                                            num_classes=VOCAB_SIZE), name='loss')


    with tf.name_scope('optimizer'):
        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)

    utils.safe_mkdir('checkpoints')

    with tf.Session() as sess:
        sess.run(iterator.initializer)
        sess.run(tf.global_variables_initializer())

        total_loss = 0.0
        writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)

        for index in range(NUM_TRAIN_STEPS):
            try:
                loss_batch, _ = sess.run([loss, optimizer])
                total_loss += loss_batch
                if (index + 1) % SKIP_STEP == 0:
                    print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))
                    total_loss = 0.0
            except tf.errors.OutOfRangeError:
                sess.run(iterator.initializer)
        writer.close()

def gen():
    yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE,
                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)

def main():
    dataset = tf.data.Dataset.from_generator(gen,
                                (tf.int32, tf.int32),
                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))
    word2vec(dataset)

if __name__ == '__main__':
    main()
```

코드가 조금더 길어졌지만, 이렇게 만듬으로써 이 모델을 재사용하기 쉬워졌다.

#### How to structure yout TensorFlow model

TensorFlow로 모델을 만드는 흐름에 대해서 다시 얘기해보자. 대부분의 코드들은 다음의 구조를 가질 것이다.

**Phase1: assemble your graph**
1. 데이터 불러오기(`tf.data` or `placeholder`)
2. 파라미터 정의
3. inference 모델 정의
4. loss 함수 정의
5. optimizer 정의

**Phase2: execute the computation**
1. 모든 변수 초기화
2. 데이터 iterator, feed 초기화
3. inference 모델 실행(각 input에 대해 학습한 결과 계산)
4. cost계산
5. 파라미터 갱신

대부분 위의 흐름을 크게 벗어나지 않을 것이다.

#### Variable Sharing
