---
layout: post
title:  "Spell Checker"
date: 2018-12-20 13:47:35 +0900
categories: NLP
tag: NLP
---
# Korean Spell Checker: 한글 데이터 맞춤법 검사

---

* Name: 조중현
* E-mail: reniew2@gmail.com  
* Github: https://gihub.com/reniew
* Blog: https://reniew.github.io

---

### Table of Contents

* [Introduction](#introduction)
* [Problem](#problem)
	* [한글 텍스트 데이터의 문제점](#한글-텍스트-데이터의-문제점)
* [전처리 Process](#전처리-process)
	* [Tokenizing](#tokenizing)
	* [한글 텍스트 띄어쓰기 모델](#한글-텍스트-띄어쓰기-모델)
	* [맞춤법 교정](#맞춤법-교정)
		* [Rule Based Checker](#rule-based-checker)
		* [Con-Vo Split](#con-vo-split)
		* [Train by Fasttext](#train-by-fasttext)
		* [Similarity and Probability Spell Checker](#similarity-and-probability-spell-checker)
* [Limitation](#limitation)
* [Conclusion](#conclusion)

---



## Introduction

스캐터랩에는 160억개 이상의 일상 대화 데이터가 존재하지만 해당 데이터를 바로 사용하기에는 맞춤법 등 몇가지 문제점이 존재하는 상황입니다. 따라서 어느정도 전처리 과정이 필요합니다. 따라서 이 글에서 일상 대화 데이터를 사용하기에 적합하도록 데이터를 전처리하는 과정에 대해서 소개하도록 합니다. 이 글의 전체 흐름은 다음과 같이 진행합니다.

* 데이터의 문제점 파악
* 전처리 Process


우선 목차에 따라서 일상 대화 데이터를 사용하는데 있어 문제점을 먼저 알아보도록 합니다.

## Problem

어떤 데이터든 사용하기 위해서는 어느정도 전처리 과정을 필요로 합니다. 또한 이미지 데이터와는 달리 텍스트 데이터의 경우 '언어'를 다루기 때문에 언어의 규칙을 지켜야 합니다. 하지만 대부분의 텍스트 데이터는 정확하게 언어의 규칙을 따르지 않기 때문에 전처리 과정을 더욱 필요로 합니다.

또한 언어의 규칙이라는 것이 각 언어마다 모두 다르기 때문에 동일한 전처리 방식을 적용할 수 없습니다. 따라서 여기서는 한글 텍스트에 집중해서 어떤 문제점이 발생하는지 알아보도록 하겠습니다.

### 한글 텍스트 데이터의 문제점

한글 텍스트 데이터에서 가장 빈번하게 발생하는 문제점은 오타가 포함되어 있다는 점입니다.

* 내일 머ㅜ 먹지?
* 입을 옷이 하나도 업어

이러한 오타에도 여러가지 유형이 있습니다. 단순히 잘못 적은 입력 실수로 인한 오타와 맞춤법을 틀린 오타 유형 그리고 수기가 아닌 기계로 텍스트를 입력할 때 발생하는 한글(영어)를 영어(한글)로 잘못 입력하는 경우입니다.

* 맞춤법
  * 점심을 굶었더니 베고파
  * 아니 도대체 외 그래
* 입력 실수
  * 그럴리가 ㅇ벗어
  * 자여넝 처리
* 영->한 / 한->영
  * anjgo -> 뭐해
  * ㅠ무뭄 -> banana

다양한 이유로 텍스트 데이터에는 오타가 존재합니다. 그리고 현재 전처리를 하려고 하는 데이터인 스캐터랩스의 데이터는 연애 관련 데이터가 많기 때문에 추가적인 문제점들이 발생할 수 있습니다.

* 'ㅇ' 붙이기
  * 뭐먹엉
  * 내일 어디 갈꺼양?
* 혀 짧은 말투
  * 아니야 안해또
* 말 늘리기
  * 뭐해애애애애
  * 학교가기 싫다아아아아아아

이렇게 연애 관련 텍스트의 경우 일반적인 텍스트보다 더 다양한 형태의 오타가 존재할 수 있습니다. 이제 이렇게 오타가 많은 텍스트 데이터를 어떤 과정으로 전처리할 수 있는지 알아보도록 하겠습니다.

## 전처리 Process

기존의 맞춤법 검사기를 사용 할 경우 새로운 단어 혹은 연애 텍스트에서 주로 발생하는 오타를 교정할 수 없다는 문제점이 있습니다. 따라서 현재 데이터에 맞는 맞춤법 교정기가 새롭게 필요한 상황입니다.

우선 맞춤법을 교정하기 위해서 가장 먼저 거쳐야 되는 과정은 Tokenizing 과정입니다. 형태소를 기준으로 Tokenizing을 해야 나눠진 토큰을 기준으로 올바른 맞춤법으로 수정할 수 있습니다. 만약 이 형태소를 기준 Tokenizing하지 않고 맞춤법을 교정한다면 맞춤법을 수정해야 하는 기준을 정확하게 찾지 못해 정확도가 떨어질 수 있습니다.

### Tokenizing

현재 데이터가 채팅 데이터이기도 하며, 신조어도 많이 포함되어 있기 때문에 기존의 형태소 분석기(ex. Twitter, Kkma 등)를 사용하기에는 어려움이 있습니다. 따라서 여기서는 김현중님께서 만드신 `soynlp`를 사용해서 토크나이징을 합니다.

`soynlp`의 `L-Tokenizer`를 사용하는데 여기서는 cohension score의 개념이 나옵니다. cohension score란 단어가 하나의 독립적으로 사용되는 단어임을 판단한는 기준이 되는 score입니다. score를 구하는 수식은 다음과 같습니다.

$$
\begin{matrix}
\text{cohension}(c_{1:n}) &=& \sqrt[n]{\prod_{i=1}^{n-1}p(c_{1:i+1}|c_{1:i})}\\ \\
&=&\sqrt[n]{p(c_{1:2}|c_{1:1})\cdot\cdot\cdot p(c_{1:n}|c_{1:n-1})}\\ \\
&=&\sqrt[n]{\frac{\text{count}(c_{1:2})}{\text{count}(c_{1:1})}\cdot\cdot\cdot \frac{\text{count}(c_{1:n})}{\text{count}(c_{1:n-1})}}\\ \\
&=&\sqrt[n]{\frac{\text{count}(c_{1:n})}{\text{count}(c_{1:1})}}
\end{matrix}
$$

위 수식으로 구한 cohension score를 구한 후 해당 점수를 활용해서 Tokenizing 하는 방식이 `L-Tokenizer`입니다. 하지만 cohension score가 잘 구해지기 위해서는 텍스트가 띄어쓰기가 잘 되어 있는 상태여야하는데, 현재 데이터는 정확한 띄어쓰기를 기대하기 힘든 상황입니다.

따라서 맞춤법 교정을 하기에 앞서, 데이터의 띄어쓰기 처리를 먼저 진행합니다. 띄어쓰기의 경우 별도의 모델을 만들어서 진행해야 합니다.

### 한글 텍스트 띄어쓰기 모델

텍스트를 올바르게 띄어쓰기가 되어있도록 만들기 위해 모델을 구현합니다. 모델에 사용될 데이터는 '한글 챗봇 데이터'([Link](https://github.com/songys/Chatbot_data))를 사용합니다. 해당 데이터를 선택한 이유는 해당 모델을 적용시킬 데이터 또한 대화 형식의 데이터이므로 최대한 비슷한 도메인의 데이터를 선택했습니다.

모델은 Stacked Bi-LSTM model 을 사용합니다. 모델의 Architecture는 다음과 같습니다.

![Untitled Diagram](https://i.imgur.com/uRQAgQr.png)

모델은 Bidirectional LSTM 2개가 쌓여있는 형태입니다. 입력값은 음절 하나가 임베딩 벡터로 변환해서 들어가게 됩니다. 두개의 LSTM을 지난 후 나온 출력값을 Dense Layer를 거쳐 차원을 변경시킵니다. 이렇게 나온 결과값에 sigmoid 함수를 적용시키면 최종결과가 0~1로 나오게 되는데 1을 띄어쓰기, 0을 띄어쓰기로 설정해 학습합니다.

우선 해당 모델을 사용하기 위해서는 데이터를 모델에 적용시킬 수 있도록 처리해줘야 합니다. 주어진 텍스트에 대해 입력값과 라벨을 다음과 같이 구성합니다.

* Text: 다시 새로 사는 게 마음 편해요
* Input: ['다', '시', '새', '로', '사', '는', '게', '마', '음', '편', '해', '요']
* Label: [0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]

해당 모델에 대한 코드는 [Github: https://github.com/reniew/Korean_Text_Spacing](https://github.com/reniew/Korean_Text_Spacing)을 올려뒀습니다.

### 맞춤법 교정

앞서 띄어쓰기 모델을 사용해서 데이터에 대한 띄어쓰기 처리를 했기 때문에 데이터가 앞서 말한 `soynlp`의 `L-Tokenizer`를 사용하기에 적합해졌습니다. 토크나이징 과정을 포함한 전체 맞춤법 교정 Process는 다음과 같습니다.

![spell](https://i.imgur.com/qbBBYSD.png)

우선 전체적인 Process의 효율와 정확도를 위해 규칙 기반으로 맞춤법을 교정합니다. 이후 해당 데이터에 약간의 처리를 해주는데 각 음절을 자,모음 단위로 나눠줍니다. 이는 Fasttext의 Subword를 자,모음으로 이용하기 위함입니다. 이후 Fasttext를 이용해 학습해준 후 해당 결과를 이용해서 Vocabulary를 만들어 줍니다. 이 결과를 이용해서 최종적으로 데이터에 대해 맞춤법 교정을 진행합니다.

#### Rule Based Checker

여기서는 자,모음로 구분지었을 때 학습에 지장을 줄 수 있는 부분과 일부 데이터에 대해 규칙 기반으로 교정을 진행합니다. 규칙은 여러번 실험을 통해서 추가될 수 있습니다. 현재 고안한 규칙은 다음과 같습니다.


* 텍스트 안에있는 URL의 경우 `<URL>` 토큰으로 만들어준다.

```python
def to_urltoken(text):

    def find_urls(text):    
        urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
        return urls

    urls = find_urls(text)
    while urls:
        text = text.replace(urls[-1], "<URL>")
        urls.pop()

    return text
```
```python
data = '여기 블로그 완전 좋음 https://reniew.github.io'
data2 = '블로그랑 깃헙 추천 해줌 https://github.com/reniew https://reniew.github.io'

print(to_urltoken(data))
print(to_urltoken(data2))
# 여기 블로그 완전 좋음 <URL>
# 블로그랑 깃헙 추천 해줌 <URL> <URL>
```


* 영어 표현에 대해 한글 단어가 존재할 경우 한글로 변경

한글 단어의 경우 기존에 사용되는 한글 Corpus를 사용해서 구축, 기본적인 것들을 우선 수정하는 과정입니다.

이 두가지 외에도 추가적으로 규칙을 정하여 추가할 수 있습니다.

#### Con-Vo Split

오타와 정확한 표현 사이의 유사도를 구하기 위해 Fasttext 기법을 사용할 것입니다. Fasttext는 단어를 subword로 나눠서 각 벡터들의 합으로 학습을 시키는 구조인데 영어에 대해서 subword로 나눌 경우 아래와 같이 오타와 정확한 표현이 겹치는 vector가 많이 존재합니다.

```
subword(where)  => (##w, #wh, whe, her, ere, re#, e##)
subword(wheer)  => (##w, #wh, whe, her, eer, er#, r##)

Prob of intersection => 4 / 7 = 0.5714
```

위와 같이 where 과 오타인 wheer의 경우 7개의 subword 중에 4개가 겹칩니다. 하지만 한글의 경우 음절을 기준으로 subword를 구하면 다음과 같이 겹치는 부분이 거의 없습니다.

```
subword(안했어)  => (##안, #안했, 안했어, 했어#, 어##)
subword(안해또)  => (##안, #안해, 안해또, 해또#, 또##)

Prob of intersection => 1 / 5 = 0.2
```

위와 같이 단어의 길이가 3인데 겹치는 벡터는 1개만 존재하게 됩니다. 따라서 각 음절을 자모 단위로 나눠준 후 subword를 구합니다.

```
안했어 => ㅇㅏㄴㅎㅐㅆㅇㅓ
안해또 => ㅇㅏㄴㅎㅐㄸㅗ

subword(ㅇㅏㄴㅎㅐㅆㅇㅓ) = (##ㅇ, #ㅇㅏ, ㅇㅏㄴ, ㅏㄴㅎ, ㄴㅎㅐ, ㅎㅐㅆ, ㅐㅆㅇ, ㅆㅇㅓ, ㅇㅓ#, ㅓ##)
subword(ㅇㅏㄴㅎㅐㄸㅗ) = (##ㅇ, #ㅇㅏ, ㅇㅏㄴ, ㅏㄴㅎ, ㄴㅎㅐ, ㅎㅐㄸ, ㅐㄸㅗ, ㄸㅗ#, ㅗ##)

Prob of intersection = 5 / 9 = 0.555
```

앞서 음절로 계산했을 때보다 0.3이나 높은 값이 나왔습니다. 따라서 Fasttext 학습을 자모 단위로 나눠서 학습합니다. 아래의 함수를 사용해서 텍스트를 자모 단위로 나눠줍니다.

```python
CHOSUNG = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']

JUNGSUNG = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']

JONGSUNG = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']

def split_korean(korea_data):

    result = []

    for token in korea_data:

        if re.match('.*[ㄱ-ㅎㅏ-ㅣ가-힣]+.*', token) is not None:


            if ord(token) > ord('가'):
                char_code = ord(token) - ord('가')
                chosung = int(char_code / 588)
                result.append(CHOSUNG[chosung])
                jungsung = int((char_code - (588 * chosung)) / 28)
                result.append(JUNGSUNG[jungsung])
                jongsung = int((char_code - (588 * chosung) - (28 * jungsung)))
                result.append(JONGSUNG[jongsung])
            else:
                result.append(token)
        else:
            result.append(token)
    return result
```
```python
print(split_korean('아니야 안해또'))

# ['ㅇ', 'ㅏ', ' ', 'ㄴ', 'ㅣ', ' ', 'ㅇ', 'ㅑ', ' ', ' ', 'ㅇ', 'ㅏ', 'ㄴ', 'ㅎ', 'ㅐ', ' ', 'ㄸ', 'ㅗ', ' ']
```

자모로 구분한 데이터에 대해서 Fasttext 학습을 진행합니다.

#### Train by Fasttext

이제 남은 과정은 Fasttext로 학습한 후 맞춤법을 교정하면 된다. 우선 Fasttext를 이용해서 학습하자.

```colsole
$ fasttext skipgram -input korea_corpus.txt -output ko_ft_model -minCount 1 -minn 3 -maxn 6 -lr 0.01 -dim 100 -ws 3 -epoch 10 -neg 20
```

학습한 모델을 다시 불러와서 맞춤법 교정과정을 진행합니다.

#### Similarity and Probability Spell Checker

데이터 안의 모든 Token(단어)들에 대해서 아래의 수식을 적용시켜 맞춤법을 교정합니다.

---

$D: \text{data}, t: \text{token} \in D$

$\text{let }S_t=\left\{\tilde{t}~\vert~\text{sim}(t,\tilde{t}) >\theta,~\forall~\tilde{t}\in D \right\}\cup\left\{t\right\}, \text{where } \theta \text{ is constant and } \text{sim}(a,b) \text{ is similarity between } a \text{ and } b$

$f(t)=
\begin{cases}
\begin{matrix}
\max(\left\{ \text{count}(\tilde{t}~\vert~\forall\tilde t\in S_t)\right\})&,\text{ if } _ {n}(S_t)>1 \\
t&, ~~ \text{otherwise }
\end{matrix}
\end{cases}$

---

우선 각 단어들에 대해서 Fasttext로 학습한 벡터로 유사도를 구한다. 이 때 유사도가 일정 임계값(threshold)을 넘는 단어들의 경우 유사 단어 집합으로 정의한 후 해당 집합에서 빈도수가 가장 큰 단어를 교정 단어로 설정해서 바꿔준다.

예를 들어 '아니야 안해또' 라는 문장에 대해서 교정을 한다면 우선 두 단어 '아니야'와 '안해또'라는 단어에 대해서 유사 단어 집합을 만든다.

>$S_{아니야} = \{$아니야, 아ㅣㄴ야, 아니얌, 아니양, $\cdots\}$
$S_{안해또} = \{$안해또, 안했어, 안해썽, 안해뗭, $\cdots\}$

유사 단어 집합의 원소들에 대해서 빈도수 값을 구한다.

>$\text{Count}(S_{아니야}) = \{$아니야: 384573, 아ㅣㄴ야: 123, 아니얌: 63841, 아니양: 59238, $\cdots\}$
$\text{Count}(S_{안해또}) = \{$안해또: 13394 , 안했어: 148520, 안해썽: 49232, 안해뗭: 194, $\cdots\}$

해당 집합에서 가장 빈도수가 큰 단어를 맞춤법이 정확한 단어라 판단해서 해당 단어로 바꿔준다.

>$f($아니야$)$ = 아니야
$f($안해또$)$ = 안했어

위와 같은 과정을 모든 데이터에 대해서 적용을 하면 어느정도 맞춤법이 맞게 전처리가 끝난다.

## Limitation

자모 단위로 구분지은 후 Fasttext로 학습한 결과 어느정도 오타와 맞는 표현 사이의 유사도를 측정해 교정이 가능하지만 아직 해당 모델에는 몇 가지 단점이 존재합니다.

우선 해당 모델의 교정 원리를 보면 맞춤법을 맞는 표현을 엄밀하게 하지 않고 단순히 가장 많이 등장한 단어로 설정했습니다. 이러한 과정은 대부분의 단어에 대해서 정확한 표현을 찾아내지만 맞지 않는 표현이 더 많은 빈도수를 가질 때 문제가 발생합니다.

저와 여자친구가 한 채팅 데이터를 기반으로 확인해 보면 이러한 문제점을 확인할 수 있습니다.

```R
print('뭐해: ',counter['뭐해'])
print('모행: ',counter['모행'])
print('모해: ',counter['모해'])
```
```
뭐해:  18
모행:  132
모해:  20
```

각 단어의 빈도수를 확인하면 올바른 표현인 '뭐해'가 18번, 오타인 '모행', '모해' 가 132번, 20번으로 더 큰 빈도수를 가지고 있습니다. 따라서 해당 모델을 사용해서 교정을 할 경우 올바르지 않은 표현으로 교정할 수 있는 위험이 존재합니다.

비록 전체 문장 수가 59630인 그렇게 크지 않은 데이터이에서 조사한 결과지만 이러한 문제가 발생할 수 있다는 가능성은 충분히 있다고 판단됩니다. 따라서 이러한 문제를 해결하기 위해서는 어느정도 정확한 표현을 가지고 있는 말뭉치(Corpus)를 같이 넣어서 학습해야 합니다.

그리고 또 다른 문제점으로는 위의 모델을 160억개 이상의 데이터에 적용하기에는 시간이 너무 많이 소요된다는 문제가 있습니다. 한 문장을 교정하기 위해서는 각 문장의 모든 단어에 대해서 유사도가 높은 단어들을 찾고 빈도수를 구해야 한다는 단점이 있습니다.

이러한 단점을 해결하기 위해 전체 데이터에 대해 해당 모델을 적용시키는 것이 아닌 일부 데이터만 사용해서 맞춤법 교정을 진행합니다. 그리고 맞춤법 교정 과정에서 **오타사전**, 즉 '**Vocabulary**'를 아래의 예시와 같이 만들어줍니다.

> {'안해또': '안했어',
 '안해떵': '안했어',
 '안해썽': '안했어',
 '아ㅣㄴ야': '아니야',
 '아ㅣㄴ야': '아니야',
'아니양': '아니야',
 '아니얌': '아니야',
...}

이렇게 만든 vocabulary를 사용해서 나머지 데이터의 맞춤법 교정을 진행합니다. 나머지 데이터에 대해 각 단어들을 Vocabulary를 참고해 올바른 표현으로 바꿔줍니다.

## Conclusion

아직 정확한 구현과 모델에 대해서 튜닝 과정도 없기 때문에 정확한 맞춤법 교정을 기대하기는 어렵지만, 최대한 현재 데이터에 맞는 교정 방법이 무었일지를 고민했습니다. 맞춤법에 대한 라벨링이 되어있지 않은 상황에서 최선은 확률값을 사용하는 것이라 판단해서 이러한 구조의 모델이 나오게 되었습니다.
