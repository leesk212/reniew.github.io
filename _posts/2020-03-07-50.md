---
layout: post
title:  "A Overview of BERT related papers"
date: 2020-03-07 13:47:35 +0900
categories: NLP
tag: NLP
---


|   Method   |   Date  |       Conference      |                        Architecture                        |                            Objective                            |                                         Dataset                                         |      Tokenizer     |  Parameters(base) |  Parameter(large) |   GLUE  | SQuAD 1.0 | SQuAD 2.0 |                                                  Link                                                 |
|:----------|-------:|:---------------------|:----------------------------------------------------------|:---------------------------------------------------------------|:---------------------------------------------------------------------------------------|:------------------|:-----------------|:-----------------|-------:|---------:|---------:|:-----------------------------------------------------------------------------------------------------|
| ELMo       | 2018/02 | NACCL 2018            | BiLSTM                                                     | LM                                                              | WikiText-103                                                                            | X                  | X                 | X                 |       X |      85.8 |         X | [Link](https://arxiv.org/pdf/1802.05365.pdf)                                                                  |
| BERT       | 2018/10 | NACCL 2019            | Transformer Encoder                                        | MLM + NSP                                                       | BookCorpus                                                                              | WordPiece          | 110M              | 340M              |    82.1 |      90.9 |      81.8 | [Link](https://arxiv.org/pdf/1810.04805.pdf)                                                                  |
| GPT        | 2018/06 | arxiv                 | Transformer Decoder                                        | LM                                                              | WebText (Reddit)                                                                        | WordPiece          | 117M              | X                 |    72.8 |         X |         X | [Link](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)         |
| GPT2       | 2019/07 | arxiv                 | Transformer Decoder                                        | LM                                                              | BookCorpus + Wiki                                                                       | Byte-Pair Encoding | 117M (제로 존재x) | 1542M             |       X |         X |         X | [Link](ttps://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |
| GPT3       | 2020/05 | arxiv                 | Transformer Decoder                                        | LM                                                              | OOO + OOO                                                                               | OOO | OOOOM (제로 존재x) | 1542M             |       X |         X |         X | [Link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |
| RoBERTa    | 2019/07 | arxiv                 | Transformer Encoder                                        | MLM                                                             | BookCorpus + Wiki + CCNews + OpenWebText + Stories                                      | Byte-Pair Encoding | 125M              | 355M              |    88.5 |      94.6 |      89.4 | [Link](https://arxiv.org/pdf/1907.11692.pdf)                                                                  |
| ALBERT     | 2019/09 | ICLR 2020 (spotlight) | Transformer Encoder                                        | MLM + SOP                                                       | Same as RoBERTa and XLNet                                                               | SentencePiece      | 12M               | 235M              |    89.4 |      94.8 |      90.2 | [Link](https://arxiv.org/pdf/1909.11942.pdf)                                                                  |
| SpanBERT   | 2019/07 | arxiv                 | Transformer Encoder                                        | SpanMLM + SBO                                                   | Same as BERT                                                                            | WordPiece          | X                 | 340M              |    82.8 |      94.6 |      88.7 | [Link](https://arxiv.org/pdf/1907.10529.pdf)                                                                  |
| StructBERT | 2019/08 | ICLR 2020 (poster)    | Transformer Encoder                                        | Word Structural Objective(+MLM) + Sentence Structural Objective | BookCorpus + Wiki                                                                       | WordPiece          | 110M              | 340M              |    83.9 |      92.0 |         X | [Link](https://arxiv.org/pdf/1908.04577.pdf)                                                                  |
| XLNet      | 2019/06 | NIPS 2019 (oral)      | Transformer Encoder  + Two-Stream Attention                | PLM                                                             | Wiki +  BookCorpus + Giga5  + ClueWeb + Common Crawl                                    | SentencePiece      | X                 | similar with BERT |    88.4 |      94.5 |      88.8 | [Link](https://arxiv.org/pdf/1906.08237.pdf)                                                                  |
| ERNIE 2.0  | 2019/07 | AAAI 2020             | Transformer Encoder                                        | KM + CP + TDRP + SR + SD + DR + IRR                             | Encyclopedia + BookCorpus + News + Dialog + IR Relevance Data + Discourse Relation Data | WordPiece          | 110M              | 340M              |    83.6 |         X |         X | [Link](https://arxiv.org/pdf/1907.12412.pdf)                                                                  |
| XLM        | 2019/01 | NIPS 2019 (spotlight) | Transformer (Transformer Encoder  or Transformer Decoder)  | CLM, MLM, TLM                                                   |  Wiki + Parellel Corpora                                                                | Byte-Pair Encoding | X                 | similar with BERT |       X |         X |         X | [Link](https://arxiv.org/pdf/1901.07291.pdf)                                                                  |
| MASS       | 2019/05 | ICML 2019 (oral)      | Transformer                                                | Seq2Seq SpanMLM                                                 | * Task dependent                                                                        | Byte-Pair Encoding | 200M(@)           | X                 |       X |         X |         X | [Link](https://arxiv.org/pdf/1910.13461.pdf)                                                                  |
| SemBERT    | 2019/09 | AAAI 2020             | Transformer Encoder  + (Conv1D + Max Pool)  + (BiGRU + FC) | MLM + NSP                                                       | Same as BERT                                                                            | WordPiece          | 110M + \alpha     | 340M + \alpha     |    82.9 |         X |      87.9 | [Link](https://arxiv.org/pdf/1909.02209.pdf)                                                                  |
| BART       | 2019/08 | EMNLP 2019            | Transformer                                                | DAE (with TM & TD & TI & SP & DR)                               | BookCorpus + CCNews + OpenWebText + Stories                                             | Byte-Pair Encoding | 150M(@)           | 400M              | 88.4(^) |      94.6 |      89.4 | [Link](https://arxiv.org/pdf/1910.13461.pdf)                                                                  |
| T5         | 2019/08 | arxiv                 | Transformer                                                | Seq2Seq SpanMLM                                                 | Colossal Clean Crawled Corpus (C4)                                                      | SentencePiece      | 220M              | 11B               | 89.7(^) |     95.64 |         X | [Link](https://arxiv.org/pdf/1910.10683.pdf)                                                                  |
| ELECTRA    | 2020/03 | ICLR 2020 (poster)    | Transformer Encoder                                        | RTD+MLM                                                         | Same as XLNet                                                                           | WordPiece          | 110M              | 335M              |    89.4 |      94.9 |      90.6 | [Link](https://arxiv.org/pdf/2003.10555.pdf)                                                                  |
<center> Table 1. Summary of BERT related papers </center>


# Introduction

최근 많은 NLP research 들의 trend를 뽑으라면 의심할 여지 없이 scalable corpus를 활용한 pre-training method를 사용하는 것이다.
2017년 Google에서 [Transformer](https://reniew.github.io/43/) 모델을 발표했고, 이 모델은 Machine Translation task에서 매우 높은 성능 향상을 가져왔다. 이후 Transformer architecture를 활용한 다양한 논문들이 나오게 되었다.
그 중 가장 주요한 논문을 하나 뽑으라면, [BERT](https://reniew.github.io/47/)를 뽑을 수 있을 것이다. BERT는 Masked Language Modeling과 Next Sentence Prediction task를 pre-training함으로써 11개의 NLP task에서 SoTA의 성능을 보여줬다. 
이후 많은 논문들이 BERT의 영향을 받았고, 좀 더 나아가서 성능을 끌어올린 다양한 논문들이 나왔다. 
따라서 BERT와 연관된(혹은 비슷한 구조를 가진) 다양한 논문들을 정리해보고 알아보도록 하자.


# A History of the Contextual Learning
NLP task들을 해결하기 위해 Pre-training 방법은 초기 deep learning 모델들부터 최근의 모델들까지 매우 다양하게 활용되어 왔다.
초창기의 pre-training 모델을은 word-level의 embedding을 training 했다.
[Word2Vec](https://reniew.github.io/21/), [GloVe](https://reniew.github.io/23/), [FastText](https://fasttext.cc/)등 초기의 pre-training 방법들은 다양한 NLP task에서 성능 향상을 가져왔음에도 불구하고 여러가지 한계점을 가졌다.
예를들면 word-level로 학습된 embedding은 해당 word의 주변 context에 independent하기 떄문에 동의어(polysemy)등을 represent하는데에 제한 적이였다. 같은 단어지만 다른 context에서 등장하는 단어들을 각각 다르게 represent되야 하는데, 이러한 부분에서 한계점을 보였고 이를 보완하기 위해 sentence, paragraph, document 자체를 embedding하는 연구들([Skip-thought vector](https://arxiv.org/abs/1405.4053), [Context2Vec](https://www.aclweb.org/anthology/K16-1006.pdf), [Doc2Vec](https://arxiv.org/pdf/1405.4053v2.pdf)) 이 제안되었다.
하지만 이러한 연구들은 문장 혹은 문서들을 context를 고려해 고정된 크기의 vector로 embedding을 한 것이지, 각각의 단어들을 context를 고려한 embedding을 하지는 못했다.

이러한 한계를 해결하기 위해 context-dependent한 embedding을 pre-training하기 위한 다양한 방법들이 제안되었다. 
CoVe, ELMo등 contextualize된 word representation을 학습하는 방법(contextual learning)들이 나왔고, 이러한 방법들은 같은 단어일지라도 다른 context에서 다른 representation을 generate했다. 
이후 Attention-based인 Transformer architecture가 제안되었고, 해당 architecture를 활용한 pre-training 모델인 GPT, BERT가 나왔다. 
해당 모델은 Transformer architecture를 활용해서 Language modeling을 학습했고, 그로 인해 대부분의 NLP task에서 SoTA의 성능을 보였다.

BERT 이후 많은 NLP 연구들이 Transformer architecture를 활용한 contextual learning에 주목을 했고, ALBERT, RoBERTa, SpanBERT, StructBERT 등 다양한 모델들이 새롭게 나오고 계속해서 SoTA의 성능을 갈아치우고 있다.


# Overview

이번 장에서 다양한 Contextual learning들에 대해서 간략하게 알아보도록 하자. 각각의 제안된 논문 기준으로 하나씩 알아보도록 할 것이며, 전체적인 contextual learning에 대한 summary는 앞서 Table1 에 요약되어 있다. 우선 각 방법들에 대해서 알아보기 이전에 알아볼 논문에서 소개된 Pre-training Objective에 대해서 먼저 정리하고 각 논문들을 하나씩 알아보도록 하자.

## Pre-training Objective



|   Objective   |   Loss  |       Inputs      |                        Targets                        |
|:----------|-------:|:---------------------|:----------------------------------------------------------|
|LM | $\mathcal{L}_{\text{LM}} = -\sum^T_{t=1}\log(p(x_t\vert \bathbf{x}_{<t})$ |c| d |
$\mathcal{L}_{\text{LM}} = -\sum_(t=1)^2$


## ELMo
[ELMo](https://arxiv.org/pdf/1802.05365.pdf)(Embeddings from Language Model)는 2018년 2월 allenNLP에서 제안된 방법론으로, 각 단어의 복잡한 특성(문법, 의미) 뿐만 아니라 언어적 context에 따라서 달라지는 부분 또한 represent한다. 
LSTM을 사용하며, Bidirectional Language Modeling(Bi-LM)을 Objective로 학습한다. 여기서 Bi-LM이란 각 방향(forward/backward)으로 각각 Language Modeling을 진행하며, 학습된 각 방향의 representation을 합쳐(concatenate) 이와 같은 architecture를 통해서 모델의 output을 bidirectional한 context를 모두 가지고 있는 representation이 된다. Pre-trained ELMo를 task-specific한 architecture에서 사용함으로써 더욱 향상된 성능을 보여준다.

## BERT
[BERT]

## GPT
## GPT2
## GPT3
## RoBERTa
## ALBERT
## SpanBERT
## StructBERT
## XLNet
## ERNIE 2.0
## XLM
## MASS
## SemBERT
## BART
## T5
## ELECTRA
