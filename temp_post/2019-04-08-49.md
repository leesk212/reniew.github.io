---
layout: post
title:  "A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss"
date: 2019-04-08 13:47:35 +0900
categories: NLP
tag: NLP
---


Text Summarization Task에 대한 모델을 제시한 논문 중 하나인 **[A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss](https://aclweb.org/anthology/P18-1013)** 에 대해서 알아보도록 하겠습니다. Abstractive한 방법과 Extractive한 두 가지 방법을 모두 합쳐 ROUGE score가 높으면서도 novel한 sentence를 만드는 모델을 구축했습니다.

### 1 Introduction

Text summarization은 자동으로 text를 중요한 의미를 보존하면서 더 짧은 text로 압축하는 Task이다. 해당 task를 통해서 news digests, presenting search results, reports를 만들 수 있다. 여기에는 두 가지 방법론이 존재하는데 하나는 Extractive한 방법이고 나머지는 Abstractive한 방법이다. Extractive한 방법이란, 기존의 Text에서 중요한 의미를 가진 단어들만을 재조합해서 짧은 문장을 생성하는 방법이다. 반대로 Abstractive한 방법이란 기존의 source text에서 단어를 가져오는 것이 아닌 해당 text의 의미를 표현하는 새로운 단어들로 짧은 문장을 generate하는 방법이다.

따라서 Abstractive한 summary들이 보통 더욱 coherent하고 concise한 경향을 보인다.

Extractive한 방법은 주로 간단하게 구현할 수 있는데, 보통 각 문장에서 해당 단어들이 요약의 결과로 사용될지에 대한 확률이 output이 된다. 많은 기존의 summarization 기법들은 주로 extractive한 방법을 주로 사용해 왔다. 그 중에서 [Nallapati et al. (2017)](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPDFInterstitial/14636/14080) 의 모델이 가장 높은 ROUGE score를 기록했다.

Extractive한 접근법들이 간단한 반면 abstractive한 접근법들을 주로 복잡한 mechanism을 가진다. Nerual network 기반의 attentional encoder-decoderr 모델들은 abstractive 하면서도 높은 ROUGE score들을 기록하는데, 그럼에도 불고하고 의미적으로 부족하거나 OOV를 다루는데 많은 어려움을 겪고있다.

최근 [See et al. (2017)](https://nlp.stanford.edu/pubs/see2017get.pdf)의 논문에서 pointer-generator model을 제시했는데, 해당 모델은 기존의 abstractive한 방법들의 단점을 극복할 수 있는 모델이다.

해당 논문에서는 state-of-the-art한 extractive, abstractive summarization의 이점들을 활용한 unified model을 제시할 것이다. 
